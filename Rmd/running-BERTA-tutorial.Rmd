---
title: "Bayesian Estimation of Recruitment Trends in Alberta (BERTA) Tutorial"
author: "Christopher L. Cahill"
date: "31 December 2021"
output:
  pdf_document: default
  html_document: default
highlight: tango
urlcolor: blue
---
# Goals
- Become familiar with the `run.R` script
- Gain intuition for how .R and .stan scripts are working together to subset data, fit a Bayesian population dynamics model to those data, and then save the model fit with a unique file name identifier.
- In particular, improve understanding and technical programming skills around tidyverse sub-setting, `get_fit()`, `future_pwalk()`, and `plan()` functions. 
- Practice debugging using `browser()`.

# TODO: 
- Description of how time index 1, 2, etc. is defined from calendar years, or calculated from some raw data file where the data are entered by calendar year
- how to add new lakes and/or year(s) of survey data to the BERTA-wide rds file
- double check goals

# Packages 
Let's load the packages we will use:

```{r}
library(tidyverse)
library(rstan)
library(furrr)
library(future) 
library(ggplot2)
```

# Data
We will work with the Fall Walleye Index Netting (FWIN) dataset used in Cahill et al. (2021), which included all Alberta lakes with $\ge$ 3 FWIN surveys during 2000-2018.  Life history parameters $\omega$, $A_{50}$, $L_{\infty}$, $vbk$, and ${\beta}_{wl}$ were obtained using hierarchical modeling methods described in Cahill et al. (2020), and these values represent lake-specific averages. 

```{r}
data <- readRDS(here::here("data/BERTA-wide-0-25.rds"))
glimpse(data)
```

Now read in the stocking data, which was used for plotting and not fitted in the .stan model.  Note these stocking records go from 1980-2018, and values represent the number of Walleye stocked per hectare: 

```{r}
stocking <- readRDS(here::here("data/stocking_matrix_ha.rds"))
glimpse(stocking)
```

# Create a wrapper function
Once the data are read into R, we can write a wrapper function called `get_fit()` that does the following:

- subsets all the data to data for a specific lake
- creates the appropriate tagged list data structures as input into the Stan model
- creates appropriate input for parameters for our Stan model 
- runs the model for a particular combination of priors (e.g., which ${\alpha}_{r}$), structural control parameters (e.g., Ricker vs. Beverton-Holt stock-recruit), and Stan run parameter values (i.e., how many iterations, warmup, chains?) 
- saves this model run with a unique identifier file name 

This may seem like a pain, but coding this way will help us later on when we need to run multiple models on different data sets.  The wrapper function is:  

```{r}
get_fit <- function(which_lake = "pigeon lake",
                    rec_ctl = c("bev-holt", "ricker"),
                    cr_prior = c(6, 12),
                    n_iter = n_iter, n_chains = n_chains,
                    n_warmup = n_iter / 2,
                    ...) {
  # Some outrageous amount of code here in the run.R script
}
```

There is too much code in this function to show it all here.  Let's break `get_fit()` down into pieces.  Please note these next lines won't run because we are jumping inside a function (thus your machine won't be able to find certain parameters). 

The first few lines of `get_fit()` are: 

```{r, eval = FALSE}

rec_ctl <- match.arg(rec_ctl)
cat(
    crayon::green(
      clisymbols::symbol$tick
    ),
    fitted = "model fitted = ", which_lake, rec_ctl, 
    sep = " "
)
cat("\n")
```

This code is ensuring that the variable rec_ctl is either "ricker" or "bev-holt" via `match_arg()`.  Next, this code is printing which lake and recruitment model is being fitted to the console via cat(). This isn't super important and realistically can be omitted or ignored by most users.

The next chunk of code is: 

```{r, eval = FALSE}
#filter the run data from all data, re-order it 
run_data <- data %>% filter(name %in% which_lake)
 run_data <-
    within(run_data, lake <-
      as.numeric(interaction(
        run_data$WBID,
        drop = TRUE, lex.order = F
    )))
run_data <- run_data[order(run_data$lake), ]
```

This first line subsets all available data via `%>%` and `filter()`, and returns data corresponding to the variable `which_lake`.  

The next few lines of code are simply re-ordering the data to ensure FWIN catch data for a given lake are in ascending order in terms of years via `within()` and `order()`  

Once we have subset the FWIN data, we subset the stocking data in a similar way:

```{r, eval = FALSE}
# stocking stuff was run in different versions, now just for plotting:
run_stocking <- stocking[which(rownames(stocking) %in% which_lake), ]
  
# Add ten years of zero for short term projections
proj_stock <- rep(0, 10) 
run_stocking <- round(c(run_stocking, proj_stock)) #add to stocking data (for plots)
```

The extra lines for `proj_stock` simply add zeros to the end of the stocking data for a given lake.  These were important when we were attempting to fit a stocking survival parameter in previous versions of the model, but are no longer used except for plotting. 

The next chunk of code in `get_fit()` is 

```{r, eval = FALSE}
# Set up the Rbar years
suppressMessages(
  survey_yrs <- run_data %>%
      group_by(lake) %>%
      summarise(
        min_yr = min(year) + length(initial_yr:(t - 1)),
        max_yr = max(year) + length(initial_yr:(t - 1))
      ) %>%
  as.numeric()
)
survey_yrs <- survey_yrs[2:3]
# summarize the life history relationships
suppressMessages(
  life_hist <- run_data %>%
    group_by(lake) %>%
    summarize(
      a50 = unique(a50),
      vbk = unique(vbk),
      linf = unique(linf),
      wl_beta = unique(beta_wl)
    )
)
```

This code is calculating the first and last FWIN survey years for the lake of interest (top chunk) and summarizing that lake's life history parameters (bottom chunk).  These chunks are wrapped in `supressMessages()` because dplyr was returning some goofy messages and it was driving me bonkers.   

Now that we have subsetted the data and calculated the necessary values from all available data, we can create a tagged data list for Stan: 

```{r, eval = FALSE}
Fseq <- seq(from = 0.01, to = 1.0, by = 0.01)

# declare the tagged data list for stan
stan_data <- list(
  n_surveys = nrow(run_data),
  n_ages = length(Ages),
  n_obs = nrow(run_data) * length(Ages),
  n_years = length(initial_yr:2028),
  n_lakes = length(unique(run_data$lake)),
  caa = run_data[, which(colnames(run_data) %in% Ages)],
  prop_aged = run_data$p_aged,
  effort = run_data$effort,
  lake = run_data$lake,
  year = run_data$year + length(initial_yr:(t - 1)),
  ages = Ages,
  survey_yrs = survey_yrs,
  which_year = 1996 - initial_yr + 2, # which integer corresponds to year = 1997
  v_prior_early = 0.3,
  v_prior_late = 0.1,
  prior_sigma_v = c(0.1, 0.5),
  R0_mean = log(6),
  R0_sd = log(3),
  ar_sd = 0.1,
  prior_mean_w = 0,
  prior_sigma_w = 1.2,
  vbk = life_hist$vbk,
  linf = life_hist$linf,
  a50 = life_hist$a50,
  wl_beta = life_hist$wl_beta,
  lbar = 57.57, # From cahill et al. 2020
  M = 0.1,
  theta = 0.85, # Lorenzen M exponent
  phi = 2.02, # vulnerability parameter (nets)
  psi = 2, # vulnerability parameter (angling)
  G_bound = c(0, Inf),
  get_SSB_obs = 1L,
  obs_cv_prior = 0.15,
  SSB_penalty = 0,
  prior_sigma_G = 1,
  Rinit_ctl = 0,
  length_Fseq = length(Fseq),
  Fseq = Fseq,
  rec_ctl = ifelse(rec_ctl == "ricker", 0, 1),
  cr_prior = cr_prior
)
```

The stuff in `stan_data` corresponds to the inputs in the data{} section of the `BERTA_singe_lake.stan` file that we will ultimately call below. Before we do that, we need to declare one more function to pass start values for our estimated parameters to Stan: 

```{r}
# create a function for start values
vk <- c(0.3, 0.3)
inits <- function() {
  list(
    v = jitter(vk, amount = 0.1),
    R0 = jitter(15, amount = 2),
    G = jitter(1, amount = 0.1),
    w = jitter(rep(
      0,
      length(1980:2028)-2
    ),
    amount = 0.1
    ),
    sigma_w = jitter(0.5, amount = 0.05),
    ar = jitter(0.5, amount = 0.01)
  )
}
```

Technically, this `inits()` function is not needed to run the model because rstan will randomly choose start values if the user does not declare them explicitly, but specifying starting values *greatly* improves the numerical performance of the chains in this particular model.  Also note that `jitter()` simply "jitters" the values by a small amount: 

```{r}
# run it once
inits()$v

# run it again and note slighly different results vs. previous call 
inits()$v
```

This is useful for initializing starting values for parameters on different chains at slightly different locations. 

Now we have successfully subset the data for a specific lake, created the appropriate `stan_data` list, and even created an `inits()` function to declare starting values for the Bayesian model.  We can now call a compiled Stan model `m` via: 

```{r, eval = FALSE}
#run the model
fit <-
  rstan::sampling(
    m,
    data = stan_data,
    pars =
      c(
        "ar_mean_kick", "F_ratio", "Fmsy", "MSY",
        "G", "cr", "ar", "SPR", "br",
        "SBR", "sbr0_kick", "R0", "v", "SSB",
        "R2", "SSB_obs", "caa_pred", "b_ratio", "w"
      ),
    iter = n_iter,
    warmup = n_warmup,
    chains = n_chains,
    init = inits,
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 15
    )
  )
```

Here, the `pars` argument simply tells Stan which parameters to monitor, `iter` determines the number of iterations to run the model for, `chains` specifies the number of chains to run, and `init` is an argument that accepts a function or list of start values for parameters in your model.

The parameters in the `control` part of `sampling()` control technical aspects of the MCMC sampling algorithm. _**We should discuss selection of the control arguments in person.**_ 

Once the stan model runs, we save a fit via: 
```{r, eval = FALSE}
# create name and save .rds files for each run
if (rec_ctl == "ricker") {
  my_name <- paste0(which_lake, "_ricker.rds")
}
if (rec_ctl == "bev-holt") {
  my_name <- paste0(which_lake, "_bh.rds")
}
stan_file <- "fits/"
stan_file <- str_c(stan_file, my_name)
stan_file <- stan_file %>% gsub(" ", "_", .)
if (cr_prior == 6) {
  stan_file <- stan_file %>% gsub(".rds", "_cr_6.rds", .)
}
if (cr_prior == 12) {
  stan_file <- stan_file %>% gsub(".rds", "_cr_12.rds", .)
}
if (file.exists(stan_file)) {
  return(NULL)
} else {
  saveRDS(fit, file = stan_file)
}
```

There is a lot of hogwash going on in here about manipulating strings via `gsub()` and `paste0()`, but what you need to know is that this chunk of code saves the Bayesian model fit to a specific .rds file in the fits folder.  These .rds files have names like `pigeon_lake_ricker_cr6.rds`.  We can call a specific .rds file for plotting or harvest control rule development or whatever later on.

That's it for `get_fit()`. 

# Functional programming with `{purrr}` and `{furrr}`

`purrr` and `furrr` are functional programming toolkits for R. A great resource on these tools is available at [https://homerhanumat.github.io/r-notes/purrr-higher-order-functions-for-iteration.html](https://homerhanumat.github.io/r-notes/purrr-higher-order-functions-for-iteration.html)

## `purrr`
This package is extremely powerful and helps with iteration.  If we wanted to iterate a simple function for multiple inputs we might go about it in base R by

- writing a simple function
- looping through that function
- examining the output of looping through that function 

We do this using a simple (albeit silly) example: 

```{r}
# let's write a silly function

silly_func <- function(x){
  result <- x + 1 
  result
}

#does the function behave as expected? 
silly_func(1)

silly_func(2)

# now let's loop through silly_func() 10 times: 
n_times <- 10 
result <- rep(NA, n_times)

for(i in 1:n_times){
 result[i] <- silly_func(i) 
}

#print the output 
print(result)

```

That works fine, now let's do this exact thing using a simple `map()` function from {purrr}.  Why we do this will make more sense in a bit.

```{r}
# create an input vector to iterate across
which_i <- 1:n_times

#iterate across that vector using map(), store as `result2`
result2 <- 
  which_i %>%
  map(silly_func)

#view the output 
print(unlist(result2))
```

A few things are important here. The first is that `map()` takes in a single argument (in our case `which_i`), and iterates `silly_func()` for each value in the vector `which_i`. It then returns a *list* corresponding to the outputs for each iteration.  This is why I used `unlist()` to print `result2`. With the exception of this list output, we can see that the for-loop above and `map()` produce the same output. Cool. 

Now, `map()` is one of the simplest {purrr} functions, but there is a whole family of useful functions in this package.  Some of these functions return lists, others return data frames, and some are even able to pass multiple values to functions that require more than a single input.  See this link and poke around to explore some of the different options available: 

[https://purrr.tidyverse.org/](https://purrr.tidyverse.org/)

The functions in {purrr} that allow users to pass more than one argument to a function are preceded by the letter 'p'. This is handy because the `get_fit()` function we wrote above has many function arguments.  Two such functions in {purrr} are called `pmap()` and `pwalk()`.  

```{r}
#create another silly function that requires two inputs and outputs their sum: 
silly_func_2 <- function(x,y){
  return(x + y)
}

# create an input vector for each x, y
input_df <- tibble(x = 1:5, y = 1:5) #tibble more or less the same as data.frame
str(input_df)

#iterate across tibble using pmap()
input_df %>%
  pmap(silly_func_2)

#iterate across tibble using pwalk()
input_df %>%
  pwalk(silly_func_2)

#note: no output
```

Thus, the 'walk' bit just means that this function is the silent analog of 'map'. Phrased differently, `pwalk()` is running exactly the same thing as `pmap()`, but it isn't storing the results anywhere or printing them out to the console.  Why might this be useful? 

# Okay, but why all this {purrr} stuff? 

So far nothing that we have done is particularly necessary or beneficial; however, when we have complex functions and lots of things to iterate across {purrr} becomes very useful. 

First, it is often faster than using for loops in R: 

```{r}
# increase amount of things to iterate across to show timing difference:
input_df <- tibble(x = 1:500, y = 1:500) 

# loop through silly_func_2 the old way and time it: 
system.time(
  for (i in 1:nrow(input_df)) {
    silly_func_2(x = input_df[i, 1], y = input_df[i, 2])
  }
)

# compare with purrr
system.time(
    input_df %>%
      pmap(silly_func_2)
)

```

So that's pretty darn cool.  Super duper speedy loops. Why is one of these faster than the other? 

The second reason you might want to code things using this functional programming logic is because there is a second package, called {furrr}, that extends most of the {purrr} commands into a 'future' supported backend.  

This means any of the {furrr} commands can be used in conjunction with the {future} package to run your iteration computations in parallel in a really easy way. This can be done using the following code: 

```{r}
# let's make the number of iterations even bigger!
input_df <- tibble(x = 1:50000, y = 1:50000) 

# loop through silly_func_2 the old way and time it: 
system.time(
  for (i in 1:nrow(input_df)) {
    silly_func_2(x = input_df[i, 1], y = input_df[i, 2])
  }
)

# set up a parallel processing plan using future and plan 
future::plan(multisession)

# compare with furrr--note the future_pmap vs. pmap from previous code chunk 
system.time(
    input_df %>%
      future_pmap(silly_func_2)
)

```

Okay, so now we are just playing dirty.  Using {furrr} we just ran the same calculation on a laptop in a fraction of the time it took to do that same calculation using a base R loop. Obviously this isn't entirely an apples to apples comparison, but the point stands that this is fast and powerful way to code.

# How does all that relate to using BERTA and `get_fit()`

Rather than looping through silly functions, how can we use this functional programming knowledge to do something useful? 

Here's how we can use all of the tricks from above to greatly speed up our computations for the BERTA assessment models: 

```{r, eval = FALSE}
# declare some indeces for stan model  
Ages <- 2:20
t <- 2000 # first survey year
max_a <- max(Ages)
rec_a <- min(Ages)
initial_yr <- t - max_a + rec_a - 2 
add_year <- initial_yr - 1

# declare HMC run parameters 
n_iter = 2000
n_chains = 4
n_warmup = n_iter/2
names <- unique(data$name)

# which lakes were the contract lakes? 
contract_lakes <- c("lac ste. anne", "baptiste lake", 
                    "pigeon lake", "calling lake", 
                    "moose lake", "lake newell"
)

# create a tibble to iterate through, where columns correspond 
# to function arguments in get_fit()
to_fit <- tibble(which_lake = contract_lakes)
to_fit$n_iter <- n_iter
to_fit$n_chains <- n_chains
to_fit$n_warmup <- n_warmup
to_fit$rec_ctl <- "ricker"
to_fit$cr_prior <- 6

to_fit2 <- to_fit
to_fit2$rec_ctl <- "bev-holt"
to_fit <- rbind(to_fit, to_fit2)

to_fit3 <- to_fit
to_fit3$cr_prior <- 12
to_fit <- rbind(to_fit, to_fit3)

str(to_fit)

# set up parallel processing plan 
future::plan(multisession)

# Run models and save fits -- 5.5 hours on my laptop
system.time({ 
  future_pwalk(to_fit, get_fit, 
               .options = furrr_options(seed = TRUE)
  ) 
})

```

Boom, we just ran 6 lakes x 2 stock-recruitment relationships x 2 informative priors for compensation ratio = 24 total Bayesian population dynamics models in only a few lines of code. We also saved each of these runs in the fits folder. 

On my laptop this takes around 8.7 hours if you don't use {furrr} and {future}, but only 5.5 hours using the functional programming sneakery.   

How could you add another lake? Or an entirely different set of lakes? 
